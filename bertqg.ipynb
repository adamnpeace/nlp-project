{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25529b09-0de3-43a5-a398-c2424a2b8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, time, random, logging, json, gc, warnings\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AdamW, BertConfig, BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import EncoderDecoderModel\n",
    "from transformers import BertModel\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import spacy\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dd9406b-bd64-4435-98c1-d1af433684ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8840c32c-6845-4206-a8ba-6a688e9f1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0ea63d53-b75d-436a-ba36-8b65d51c779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = 'bert-large-uncased'\n",
    "model_path = Path('./models')\n",
    "data_path = Path('../data')\n",
    "bert_path = model_path / bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "358d7f24-da73-4866-be57-02a461aa922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (model_path / bert_model / 'config.json').is_file():\n",
    "    BertModel.from_pretrained(bert_model).save_pretrained(model_path / bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "610d0ea5-84b9-4b23-96cf-ecf9694387f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, n_h_enc, n_h_dec, n_h_attention):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(n_h_enc + n_h_dec, n_h_attention)\n",
    "        self.v = nn.Parameter(torch.rand(n_h_attention), requires_grad=True)\n",
    "        \n",
    "    def forward(self, key, queries):\n",
    "        batch_size, src_len = queries.shape\n",
    "        \n",
    "        key = key.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat([key, queries], dim=2)))\n",
    "        \n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(2)\n",
    "        \n",
    "        attn = torch.bmm(energy, v).squeeze(2)\n",
    "        \n",
    "        return F.softmax(attn, dim=1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_dims, emb_dims, n_h_enc, n_h_dec, num_layers, dropout, attn):\n",
    "        super().__init__()\n",
    "        self.out_dims = out_dims\n",
    "        self.emb_dims = emb_dims\n",
    "        self.n_h_enc = n_h_enc\n",
    "        self.n_h_dec = n_h_dec\n",
    "        self.n_layers = num_layers\n",
    "        self.attn = attn\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(out_dims, emb_dims)\n",
    "        \n",
    "        self.nn = nn.GRU(emb_dims, n_h_dec, batch_first=True, num_layers=num_layers, dropout=dropout)\n",
    "        \n",
    "    def forward(self, src, queries, hidden):\n",
    "        src = src.unsqueeze(1)\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        out, hidden = self.rnn(embedded, hidden)\n",
    "        out = out.squeeze()\n",
    "        \n",
    "        a = self.attn(out, queries)\n",
    "        a = a.unsqueeze(1)\n",
    "        weighted = torch.bmm(a, queries)\n",
    "        \n",
    "        weighted = weighted.squeeze(1)\n",
    "        \n",
    "        out = self.out(torch.cat([out, weighted], dim=1))\n",
    "        \n",
    "        return out, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, encoder_trained):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.encoder_trained = encoder_trained\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        input_ids, token_ids, attn_mask = src\n",
    "        \n",
    "        if self.encoder_trained:\n",
    "            bert_hs = self.encoder(input_ids, token_type_ids=token_ids, attention_mask=attn_mask)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                bert_hs = self.encoder(input_ids, token_type_ids=token_ids, attention_mask=attn_mask)\n",
    "        \n",
    "        bert_encodings = bert_hs[0]\n",
    "        \n",
    "        batch_size, max_len = trg.shape\n",
    "        \n",
    "        trg_vocab_size = self.decoder.out_dims\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, max_len, trg_vocab_size).to(device)\n",
    "        \n",
    "        out = trg[:, 0]\n",
    "        \n",
    "        hidden = torch.zeros(self.decoder.num_layers, out.shape[0], self.decoder.n_h_dec).to(device)\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            out, hidden = self.decoder(out, bert_encodings, hidden)\n",
    "            outputs[:, t] = out\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            out = (trg[:, t] if teacher_force else output.max(1)[1])\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30253a22-4652-4a6c-b771-c7ff85dee587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo below\n",
    "def enable_reproducibility(seed=69):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "def no_grad(model):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    return model\n",
    "\n",
    "def no_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def no_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def model_size(model):\n",
    "    return sum(p.element_size() * p.nelement() for p in model.parameters())\n",
    "\n",
    "def save_checkpoint(name, epoch, model, optimizer, valid_loss, train_loss):\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'valid_loss': valid_loss,\n",
    "            'train_loss': train_loss,\n",
    "            }, name)\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "    return checkpoint['epoch'], checkpoint['model_state_dict'],\\\n",
    "           checkpoint['optimizer_state_dict'], checkpoint['valid_loss'], checkpoint['train_loss']\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_secs = end_time - start_time\n",
    "    elapsed_mins = elapsed_secs / 60\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b55798b4-d551-443a-89c8-8c241c3ecaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "def train(model, device, dataloader, optimizer, criterion, clip):\n",
    "    log = logging.getLogger(__name__)\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    start = time.time()\n",
    "    for i, (input_, output_) in enumerate(dataloader):\n",
    "\n",
    "        input_data, input_length = input_\n",
    "        output_data, output_length = output_\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction = model([x.to(device) for x in input_data],  output_data.to(device))\n",
    "\n",
    "        trg_sent_len = prediction.size(1)\n",
    "\n",
    "        prediction = prediction[:, 1:].contiguous().view(-1, prediction.shape[-1])\n",
    "        output_data = output_data[:, 1:].contiguous().view(-1)  # Find a way to avoid calling contiguous\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pw_loss = pw_criterion(prediction,  output_data.to(device))\n",
    "\n",
    "        loss = criterion(prediction,  output_data.to(device))\n",
    "\n",
    "        # reshape to [trg sent len - 1, batch size]\n",
    "        loss = loss.view(-1, trg_sent_len - 1)\n",
    "        loss = loss.sum(1)\n",
    "        loss = loss.mean(0)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.decoder.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % int(len(dataloader) * 0.1) == int(len(dataloader) * 0.1) - 1:\n",
    "            log.info(\n",
    "                f'Batch {i} Sentence loss {loss.item()} Word loss {pw_loss.item()}   Time: {epoch_time(start, time.time())}')\n",
    "            start = time.time()\n",
    "\n",
    "        epoch_loss += pw_loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c51e0d74-7cb0-4334-85fc-676ab4d4212c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9d2e40230c4eac8a15635a65183786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10ec47d62f44112bc617aec8256bf8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15524ce240804ebbba54940f3b852cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "def bleu_score(prediction, ground_truth):\n",
    "    prediction = prediction.max(2)[1]\n",
    "    acc_bleu = 0\n",
    "\n",
    "    for x, y in zip(ground_truth, prediction):\n",
    "        x = tokenizer.convert_ids_to_tokens(x.tolist())\n",
    "        y = tokenizer.convert_ids_to_tokens(y.tolist())\n",
    "        idx1 = x.index('[PAD]') if '[PAD]' in x else len(x)\n",
    "        idx2 = y.index('[SEP]') if '[SEP]' in y else len(y)\n",
    "\n",
    "        acc_bleu += bleu([x[1:idx1 - 1]], y[1:idx2 - 1], smoothing_function=SmoothingFunction().method4)\n",
    "    return acc_bleu / prediction.size(0)\n",
    "\n",
    "def eval(model, device, dataloader, criterion):\n",
    "    log = logging.getLogger(__name__)\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_bleu = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (input_, output_) in enumerate(dataloader):\n",
    "\n",
    "            input_data, input_length = input_\n",
    "            output_data, output_length = output_\n",
    "\n",
    "            prediction = model([x.to(device) for x in input_data], output_data.to(device), 0)  # turn off teacher forcing\n",
    "\n",
    "            sample_t = tokenizer.convert_ids_to_tokens(output_data[0].tolist())\n",
    "            sample_p = tokenizer.convert_ids_to_tokens(prediction[0].max(1)[1].tolist())\n",
    "            idx1 = sample_t.index('[PAD]') if '[PAD]' in sample_t else len(sample_t)\n",
    "            idx2 = sample_p.index('[SEP]') if '[SEP]' in sample_p else len(sample_p)\n",
    "\n",
    "            bleu = bleu_score(prediction, output_data.to(device))\n",
    "\n",
    "            trg_sent_len = prediction.size(1)\n",
    "            # trg = [trg sent len, batch size]\n",
    "            # output = [trg sent len, batch size, output dim]\n",
    "\n",
    "            prediction = prediction[:, 1:].contiguous().view(-1, prediction.shape[-1])\n",
    "            output_data = output_data[:, 1:].contiguous().view(-1)  # Find a way to avoid calling contiguous\n",
    "\n",
    "            # trg = [(trg sent len - 1) * batch size]\n",
    "            # output = [(trg sent len - 1) * batch size, output dim]\n",
    "\n",
    "            pw_loss = pw_criterion(prediction, output_data.to(device))\n",
    "\n",
    "            loss = criterion(prediction, output_data.to(device))\n",
    "            loss = loss.view(-1, trg_sent_len - 1)\n",
    "            loss = loss.sum(1)\n",
    "            loss = loss.mean(0)\n",
    "\n",
    "            if i % int(len(dataloader) * 0.1) == int(len(dataloader) * 0.1) - 1:\n",
    "                log.info(f'Batch {i} Sentence loss: {loss.item()} Word loss: {pw_loss.item()} BLEU score: {bleu}\\n'\n",
    "                         f'Target {sample_t[1:idx1-1]}\\n'\n",
    "                         f'Prediction {sample_p[1:idx2-1]}\\n\\n')\n",
    "\n",
    "            epoch_loss += pw_loss.item()\n",
    "            epoch_bleu += bleu\n",
    "\n",
    "        return epoch_loss / len(dataloader), epoch_bleu / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a27734d-bc5d-4a6a-ae53-474908486b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5f214c24-992a-45e9-9d03-fbbb9bf22722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, json_path, bert_model):\n",
    "        all_data = json.load(open(json_path, 'r'))\n",
    "        input, output = self._extract_data(all_data)\n",
    "        self.data = self._tokenize_data(input, output, bert_model)\n",
    "\n",
    "    def _extract_data(self, all_data):\n",
    "        input, output = [], []\n",
    "        for data in all_data:\n",
    "            input.append((data['passages'], data['responses']))\n",
    "            output.append(data['clues'])\n",
    "        return input, output\n",
    "\n",
    "    def _tokenize_data(self, input, output, bert_model):\n",
    "        tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "        \n",
    "        data = tokenizer.batch_encode_plus(input, pad_to_max_length=True, return_tensors='pt')\n",
    "        out_dict = tokenizer.batch_encode_plus(output, pad_to_max_length=True, return_tensors='pt')\n",
    "        \n",
    "        data['output_ids'] = out_dict['input_ids']\n",
    "        data['output_len'] = out_dict['attention_mask'].sum(dim=1)\n",
    "        data['input_len'] = data['attention_mask'].sum(dim=1)\n",
    "        \n",
    "        idx = (data['input_len'] <= LENGTH)\n",
    "        in_m = max(data['input_len'][idx])\n",
    "        out_m = max(data['output_len'][idx])\n",
    "        \n",
    "        data['input_ids'] = data['input_ids'][idx, :in_m]\n",
    "        data['attention_mask'] = data['attention_mask'][idx, :in_m]\n",
    "        data['token_type_ids'] = data['token_type_ids'][idx, :in_m]\n",
    "        data['input_len'] = data['input_len'][idx]\n",
    "        \n",
    "        data['output_ids'] = data['output_ids'][idx, :out_m]\n",
    "        data['output_len'] = data['output_len'][idx]\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data['input_ids'].shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (((self.data['input_ids'][idx],\n",
    "                self.data['attention_mask'][idx],\n",
    "                self.data['token_type_ids'][idx]),\n",
    "                self.data['input_len'][idx]),\n",
    "                (self.data['output_ids'][idx],\n",
    "                self.data['output_len'][idx])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d6c1c264-e140-4367-8e46-7566f045309e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17ee286f6554ff2b5646ef2c53cd516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99b67916e2341318203820427c0f08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039c006c8b8c4151a6af6d2505122bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "train_set = BertDataset('data_train.json', bert_model)\n",
    "dev_set = BertDataset('data_dev.json', bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a7a88c22-675d-4246-8c4c-0341ca7a0fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HP:\n",
    "    def __init__(self):\n",
    "        self.num_workers = 0\n",
    "        self.pin_memory = False\n",
    "        self.batch_size = 4\n",
    "        self.weight_decay = 0.001\n",
    "        self.lr = 0.05\n",
    "        self.decoder_hidden_size = 512\n",
    "        self.decoder_input_size = 512\n",
    "        self.attention_hidden_size = 512\n",
    "        self.n_layers = 1\n",
    "        self.clip = 1\n",
    "        self.dropout = 1\n",
    "        self.epochs = 4\n",
    "        self.mb = 32\n",
    "        self.checkpoint = None\n",
    "        self.encoder_trained = False\n",
    "        \n",
    "        conf_file = json.load(open((bert_path / 'config.json'), 'r'))\n",
    "        self.bert_hidden_size = conf_file['hidden_size']\n",
    "        self.bert_vocab_size = conf_file['vocab_size']\n",
    "        \n",
    "        \n",
    "hp = HP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "efa510eb-1da8-4aa9-830c-1e6193dd5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=hp.num_workers, pin_memory=hp.pin_memory\n",
    "                         )\n",
    "dev_loader = DataLoader(dev_set, batch_size=hp.batch_size, shuffle=True,\n",
    "                        num_workers=hp.num_workers, pin_memory=hp.pin_memory\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bd5dee48-3a8b-4509-a59b-b8a513fed6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = Attention(hp.bert_hidden_size, hp.decoder_hidden_size, hp.attention_hidden_size)\n",
    "decoder = Decoder(hp.bert_vocab_size, hp.decoder_input_size, hp.bert_hidden_size,\n",
    "                  hp.decoder_hidden_size, hp.n_layers, hp.dropout, attn)\n",
    "encoder = BertModel.from_pretrained(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1144c-86cd-47b7-8478-cbb0f73b9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05a2ae69-61d7-4242-94a6-403cd1f03bdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'bert_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-b390e858c67e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/bert/toy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#train_set, batch_size=mb, shuffle=True, num_workers=dl_workers, pin_memory=True if device=='cuda' else False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'bert_model'"
     ]
    }
   ],
   "source": [
    "ds = BertDataset('../../data/bert/toy')\n",
    "#train_set, batch_size=mb, shuffle=True, num_workers=dl_workers, pin_memory=True if device=='cuda' else False\n",
    "dl = DataLoader(ds, batch_size=8, num_workers=1, shuffle=True, pin_memory=True)\n",
    "src, dest = next(iter(dl))\n",
    "src, src_len = src\n",
    "\n",
    "print(src[0][0].shape)\n",
    "print(src[1][0].shape)\n",
    "print(src[2][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4677086a-1a77-478b-bee9-8c1cf5bed5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ...\n",
    "dev_set = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24af4bfa-a66e-4aee-a3b6-f198d7643f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = ...\n",
    "dev_loader = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b106ce-d628-4dfe-92fb-3051710ca2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31ab9539-0339-499a-9b41-0d501b8f0f91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_hidden_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0fa8e59e0a19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_hidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m decoder = Decoder(bert_vocab_size, decoder_input_size, bert_hidden_size, decoder_hidden_size, num_layers,\n\u001b[1;32m      3\u001b[0m                  dropout, attention)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bert_hidden_size' is not defined"
     ]
    }
   ],
   "source": [
    "attention = Attention(bert_hidden_size, decoder_hidden_size, attn_hidden_size)\n",
    "decoder = Decoder(bert_vocab_size, decoder_input_size, bert_hidden_size, decoder_hidden_size, num_layers,\n",
    "                 dropout, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33404799-1657-425e-8c74-8b0e72b33b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = BertModel.from_pretrained('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bf387-8ed9-4be1-8012-407346005f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder, decoder, encoder_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00c28891-1a1e-4852-8543-1692d40898be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b897ab415316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparamters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(decoder.paramters(), weight_decay=weight_decay, lr=lr, momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4edb83-8521-4484-ad26-cf44cff656c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint is not None:\n",
    "    last_epoch, model_dict, optim_dict, valid_loss_list, train_loss_list = load_checkpoint(checkpoint)\n",
    "    last_epoch += 1\n",
    "    model.load_state_dict(model_dict)\n",
    "    \n",
    "    best_valid_loss = min(valid_loss_list)\n",
    "    \n",
    "    optimizer.load_state_dict(optim_dict)\n",
    "    \n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QA-nlp",
   "language": "python",
   "name": "qa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
