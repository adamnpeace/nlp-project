{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Ws84SMCIGvqN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ws84SMCIGvqN",
    "outputId": "ada55843-7217-4cc7-d7a5-2ef7402d6d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 10 00:56:57 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3080    On   | 00000000:07:00.0 Off |                  N/A |\n",
      "| 76%   71C    P0   143W / 320W |     17MiB / 10015MiB |     21%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1002      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      1226      G   /usr/bin/gnome-shell                6MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "niSDQPa-GPyt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "niSDQPa-GPyt",
    "outputId": "cb76ccf8-f307-43bf-e28e-e32943647b08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (2.11.0)\n",
      "Requirement already satisfied: numpy in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from transformers) (1.20.2)\n",
      "Requirement already satisfied: sacremoses in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: filelock in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from transformers) (4.60.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: sentencepiece in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from transformers) (0.1.95)\n",
      "Requirement already satisfied: packaging in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: requests in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: click in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /home/dojo/.pyenv/versions/3.7.10/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/home/dojo/.pyenv/versions/3.7.10/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8840c32c-6845-4206-a8ba-6a688e9f1abd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8840c32c-6845-4206-a8ba-6a688e9f1abd",
    "outputId": "c5d73c3d-6206-4f24-c9a4-1f31c46137a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-10 00:56:59--  https://raw.githubusercontent.com/adamnpeace/nlp-project/model-testing-bertqg/data/preprocessed/data_dev.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1825775 (1.7M) [text/plain]\n",
      "Saving to: ‘data_dev.json.5’\n",
      "\n",
      "data_dev.json.5     100%[===================>]   1.74M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2021-05-10 00:56:59 (17.6 MB/s) - ‘data_dev.json.5’ saved [1825775/1825775]\n",
      "\n",
      "--2021-05-10 00:57:00--  https://raw.githubusercontent.com/adamnpeace/nlp-project/model-testing-bertqg/data/preprocessed/data_train.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7332522 (7.0M) [text/plain]\n",
      "Saving to: ‘data_train.json.5’\n",
      "\n",
      "data_train.json.5   100%[===================>]   6.99M  17.8MB/s    in 0.4s    \n",
      "\n",
      "2021-05-10 00:57:00 (17.8 MB/s) - ‘data_train.json.5’ saved [7332522/7332522]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/adamnpeace/nlp-project/model-testing-bertqg/data/preprocessed/data_dev.json\n",
    "!wget https://raw.githubusercontent.com/adamnpeace/nlp-project/model-testing-bertqg/data/preprocessed/data_train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25529b09-0de3-43a5-a398-c2424a2b8334",
   "metadata": {
    "id": "25529b09-0de3-43a5-a398-c2424a2b8334"
   },
   "outputs": [],
   "source": [
    "import os, pickle, time, random, logging, json, gc, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AdamW, BertConfig, BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import EncoderDecoderModel\n",
    "from transformers import BertModel\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate import bleu\n",
    "import spacy\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd9406b-bd64-4435-98c1-d1af433684ba",
   "metadata": {
    "id": "3dd9406b-bd64-4435-98c1-d1af433684ba"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea63d53-b75d-436a-ba36-8b65d51c779d",
   "metadata": {
    "id": "0ea63d53-b75d-436a-ba36-8b65d51c779d"
   },
   "outputs": [],
   "source": [
    "bert_model = 'bert-large-uncased'\n",
    "model_path = Path('./models')\n",
    "data_path = Path('.')\n",
    "bert_path = model_path / bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "358d7f24-da73-4866-be57-02a461aa922c",
   "metadata": {
    "id": "358d7f24-da73-4866-be57-02a461aa922c"
   },
   "outputs": [],
   "source": [
    "if not (model_path / bert_model / 'config.json').is_file():\n",
    "    BertModel.from_pretrained(bert_model).save_pretrained(model_path / bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "610d0ea5-84b9-4b23-96cf-ecf9694387f2",
   "metadata": {
    "id": "610d0ea5-84b9-4b23-96cf-ecf9694387f2"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, n_h_enc, n_h_dec, n_h_attention):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(n_h_enc + n_h_dec, n_h_attention)\n",
    "        self.v = nn.Parameter(torch.rand(n_h_attention), requires_grad=True)\n",
    "        \n",
    "    def forward(self, key, queries):\n",
    "        batch_size, src_len = queries.shape[:2]\n",
    "        \n",
    "        key = key.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        if key.size(0) != queries.size(0) or key.size(1) != queries.size(1):\n",
    "            print(key.shape, queries.shape)\n",
    "        kitten = torch.cat((key, queries), dim=2)\n",
    "        energy = torch.tanh(self.attn(kitten))\n",
    "        \n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(2)\n",
    "        \n",
    "        attn = torch.bmm(energy, v).squeeze(2)\n",
    "        \n",
    "        return F.softmax(attn, dim=1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_dims, emb_dims, n_h_enc, n_h_dec, n_layers, dropout, attn):\n",
    "        super().__init__()\n",
    "        self.out_dims = out_dims\n",
    "        self.emb_dims = emb_dims\n",
    "        self.n_h_enc = n_h_enc\n",
    "        self.n_h_dec = n_h_dec\n",
    "        self.n_layers = n_layers\n",
    "        self.attn = attn\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(out_dims, emb_dims)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dims, n_h_dec, batch_first=True, num_layers=n_layers, dropout=dropout)\n",
    "\n",
    "        self.out_dense = nn.Linear(n_h_enc + n_h_dec, out_dims)\n",
    "        \n",
    "    def forward(self, src, queries, hidden):\n",
    "        src = src.unsqueeze(1)\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        out, hidden = self.rnn(embedded, hidden)\n",
    "        out = out.squeeze()\n",
    "        \n",
    "        a = self.attn(out, queries)\n",
    "        a = a.unsqueeze(1)\n",
    "        weighted = torch.bmm(a, queries)\n",
    "        \n",
    "        weighted = weighted.squeeze(1)\n",
    "        \n",
    "        out = self.out_dense(torch.cat([out, weighted], dim=1))\n",
    "        \n",
    "        return out, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, encoder_trained):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.encoder_trained = encoder_trained\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        input_ids, token_ids, attn_mask = src\n",
    "        \n",
    "        if self.encoder_trained:\n",
    "            bert_hs = self.encoder(input_ids, token_type_ids=token_ids, attention_mask=attn_mask)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                bert_hs = self.encoder(input_ids, token_type_ids=token_ids, attention_mask=attn_mask)\n",
    "        \n",
    "        bert_encodings = bert_hs[0]\n",
    "        \n",
    "        batch_size, max_len = trg.shape[:2]\n",
    "        \n",
    "        trg_vocab_size = self.decoder.out_dims\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, max_len, trg_vocab_size).to(device)\n",
    "        \n",
    "        out = trg[:, 0]\n",
    "        \n",
    "        hidden = torch.zeros(self.decoder.n_layers, out.shape[0], self.decoder.n_h_dec).to(device)\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            out, hidden = self.decoder(out, bert_encodings, hidden)\n",
    "            outputs[:, t] = out\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            out = (trg[:, t] if teacher_force else out.max(1)[1])\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30253a22-4652-4a6c-b771-c7ff85dee587",
   "metadata": {
    "id": "30253a22-4652-4a6c-b771-c7ff85dee587"
   },
   "outputs": [],
   "source": [
    "# redo below\n",
    "def enable_reproducibility(seed=69):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "def no_grad(model):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    return model\n",
    "\n",
    "def no_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def no_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def model_size(model):\n",
    "    return sum(p.element_size() * p.nelement() for p in model.parameters())\n",
    "\n",
    "def save_checkpoint(name, epoch, model, optimizer, valid_loss, train_loss):\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'valid_loss': valid_loss,\n",
    "            'train_loss': train_loss,\n",
    "            }, name)\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "    return checkpoint['epoch'], checkpoint['model_state_dict'],\\\n",
    "           checkpoint['optimizer_state_dict'], checkpoint['valid_loss'], checkpoint['train_loss']\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_secs = end_time - start_time\n",
    "    elapsed_mins = elapsed_secs / 60\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b55798b4-d551-443a-89c8-8c241c3ecaef",
   "metadata": {
    "id": "b55798b4-d551-443a-89c8-8c241c3ecaef"
   },
   "outputs": [],
   "source": [
    "pw_criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, clip):\n",
    "    log = logging.getLogger(__name__)\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    start = time.time()\n",
    "    for i, (input_, output_) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "\n",
    "        input_data, input_length = input_\n",
    "        output_data, output_length = output_\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        prediction = model([x.to(device) for x in input_data],  output_data.to(device))\n",
    "\n",
    "        trg_sent_len = prediction.size(1)\n",
    "\n",
    "        prediction = prediction[:, 1:].contiguous().view(-1, prediction.shape[-1])\n",
    "        output_data = output_data[:, 1:].contiguous().view(-1)  # Find a way to avoid calling contiguous\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pw_loss = pw_criterion(prediction,  output_data.to(device))\n",
    "\n",
    "        loss = criterion(prediction,  output_data.to(device))\n",
    "\n",
    "        # reshape to [trg sent len - 1, batch size]\n",
    "        loss = loss.view(-1, trg_sent_len - 1)\n",
    "        loss = loss.sum(1)\n",
    "        loss = loss.mean(0)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.decoder.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % int(len(dataloader) * 0.1) == int(len(dataloader) * 0.1) - 1:\n",
    "            print('Batch {}: Sentence Loss: {:.3f}, Word Loss: {:.3f}'\n",
    "                  .format(i, loss.item(), pw_loss.item()))\n",
    "            start = time.time()\n",
    "\n",
    "        epoch_loss += pw_loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c51e0d74-7cb0-4334-85fc-676ab4d4212c",
   "metadata": {
    "id": "c51e0d74-7cb0-4334-85fc-676ab4d4212c"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "\n",
    "def bleu_score(prediction, ground_truth):\n",
    "    prediction = prediction.max(2)[1]\n",
    "    acc_bleu = 0\n",
    "\n",
    "    for x, y in zip(ground_truth, prediction):\n",
    "        x = tokenizer.convert_ids_to_tokens(x.tolist())\n",
    "        y = tokenizer.convert_ids_to_tokens(y.tolist())\n",
    "        idx1 = x.index('[PAD]') if '[PAD]' in x else len(x)\n",
    "        idx2 = y.index('[SEP]') if '[SEP]' in y else len(y)\n",
    "\n",
    "        acc_bleu += bleu([x[1:idx1 - 1]], y[1:idx2 - 1], smoothing_function=SmoothingFunction().method4)\n",
    "    return acc_bleu / prediction.size(0)\n",
    "\n",
    "def eval(model, dataloader, criterion):\n",
    "    log = logging.getLogger(__name__)\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_bleu = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (input_, output_) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "\n",
    "            input_data, input_length = input_\n",
    "            output_data, output_length = output_\n",
    "\n",
    "            prediction = model([x.to(device) for x in input_data], output_data.to(device), 0)  # turn off teacher forcing\n",
    "\n",
    "            sample_t = tokenizer.convert_ids_to_tokens(output_data[0].tolist())\n",
    "            sample_p = tokenizer.convert_ids_to_tokens(prediction[0].max(1)[1].tolist())\n",
    "            idx1 = sample_t.index('[PAD]') if '[PAD]' in sample_t else len(sample_t)\n",
    "            idx2 = sample_p.index('[SEP]') if '[SEP]' in sample_p else len(sample_p)\n",
    "\n",
    "            bleu_i = bleu_score(prediction, output_data.to(device))\n",
    "\n",
    "            trg_sent_len = prediction.size(1)\n",
    "            # trg = [trg sent len, batch size]\n",
    "            # output = [trg sent len, batch size, output dim]\n",
    "\n",
    "            prediction = prediction[:, 1:].contiguous().view(-1, prediction.shape[-1])\n",
    "            output_data = output_data[:, 1:].contiguous().view(-1)  # Find a way to avoid calling contiguous\n",
    "\n",
    "            # trg = [(trg sent len - 1) * batch size]\n",
    "            # output = [(trg sent len - 1) * batch size, output dim]\n",
    "\n",
    "            pw_loss = pw_criterion(prediction, output_data.to(device))\n",
    "\n",
    "            loss = criterion(prediction, output_data.to(device))\n",
    "            loss = loss.view(-1, trg_sent_len - 1)\n",
    "            loss = loss.sum(1)\n",
    "            loss = loss.mean(0)\n",
    "\n",
    "            if i % int(len(dataloader) * 0.1) == int(len(dataloader) * 0.1) - 1:\n",
    "                print('Batch {}: Sentence Loss: {:.3f}, Word Loss: {:.3f}, BLEU score {:.4f}'.format(i, loss.item(), pw_loss.item(), bleu_i))\n",
    "\n",
    "            epoch_loss += pw_loss.item()\n",
    "            epoch_bleu += bleu_i\n",
    "\n",
    "        return epoch_loss / len(dataloader), epoch_bleu / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a27734d-bc5d-4a6a-ae53-474908486b69",
   "metadata": {
    "id": "1a27734d-bc5d-4a6a-ae53-474908486b69"
   },
   "outputs": [],
   "source": [
    "LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f214c24-992a-45e9-9d03-fbbb9bf22722",
   "metadata": {
    "id": "5f214c24-992a-45e9-9d03-fbbb9bf22722",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, json_path, bert_model):\n",
    "        all_data = json.load(open(json_path, 'r'))\n",
    "        input, output = self._extract_data(all_data)\n",
    "        self.data = self._tokenize_data(input, output, bert_model)\n",
    "\n",
    "    def _extract_data(self, all_data):\n",
    "        input, output = [], []\n",
    "        for data in all_data:\n",
    "            input.append((data['passages'], data['responses']))\n",
    "            output.append(data['clues'])\n",
    "        input, output = input[:-1], output[:-1]\n",
    "        return input, output\n",
    "\n",
    "    def _tokenize_data(self, input, output, bert_model):\n",
    "        tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "        \n",
    "        data = tokenizer.batch_encode_plus(input, pad_to_max_length=True, return_tensors='pt')\n",
    "        out_dict = tokenizer.batch_encode_plus(output, pad_to_max_length=True, return_tensors='pt')\n",
    "        \n",
    "        data['output_ids'] = out_dict['input_ids']\n",
    "        data['output_len'] = out_dict['attention_mask'].sum(dim=1)\n",
    "        data['input_len'] = data['attention_mask'].sum(dim=1)\n",
    "        \n",
    "        idx = (data['input_len'] <= LENGTH)\n",
    "        in_m = max(data['input_len'][idx])\n",
    "        out_m = max(data['output_len'][idx])\n",
    "        \n",
    "        data['input_ids'] = data['input_ids'][idx, :in_m]\n",
    "        data['attention_mask'] = data['attention_mask'][idx, :in_m]\n",
    "        data['token_type_ids'] = data['token_type_ids'][idx, :in_m]\n",
    "        data['input_len'] = data['input_len'][idx]\n",
    "        \n",
    "        data['output_ids'] = data['output_ids'][idx, :out_m]\n",
    "        data['output_len'] = data['output_len'][idx]\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data['input_ids'].shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (((self.data['input_ids'][idx],\n",
    "                self.data['attention_mask'][idx],\n",
    "                self.data['token_type_ids'][idx]),\n",
    "                self.data['input_len'][idx]),\n",
    "                (self.data['output_ids'][idx],\n",
    "                self.data['output_len'][idx])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6c1c264-e140-4367-8e46-7566f045309e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6c1c264-e140-4367-8e46-7566f045309e",
    "outputId": "7eb30c96-4107-4294-8bb3-5da0520fe9cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "train_set = BertDataset('data_train.json', bert_model)\n",
    "dev_set = BertDataset('data_dev.json', bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7a88c22-675d-4246-8c4c-0341ca7a0fd0",
   "metadata": {
    "id": "a7a88c22-675d-4246-8c4c-0341ca7a0fd0"
   },
   "outputs": [],
   "source": [
    "class HP:\n",
    "    def __init__(self):\n",
    "        self.num_workers = 0\n",
    "        self.pin_memory = False\n",
    "        self.batch_size = 4\n",
    "        self.weight_decay = 0.001\n",
    "        self.lr = 0.05\n",
    "        self.momentum = 0.9\n",
    "        self.decoder_hidden_size = 512\n",
    "        self.decoder_input_size = 512\n",
    "        self.attention_hidden_size = 512\n",
    "        self.n_layers = 1\n",
    "        self.clip = 1\n",
    "        self.dropout = 1\n",
    "        self.n_epochs = 4\n",
    "        self.mb = 32\n",
    "        self.checkpoint = None\n",
    "        self.encoder_trained = False\n",
    "        \n",
    "        conf_file = json.load(open((bert_path / 'config.json'), 'r'))\n",
    "        self.bert_hidden_size = conf_file['hidden_size']\n",
    "        self.bert_vocab_size = conf_file['vocab_size']\n",
    "        \n",
    "        \n",
    "hp = HP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efa510eb-1da8-4aa9-830c-1e6193dd5e10",
   "metadata": {
    "id": "efa510eb-1da8-4aa9-830c-1e6193dd5e10"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=hp.batch_size, shuffle=True,\n",
    "                          num_workers=hp.num_workers, pin_memory=hp.pin_memory\n",
    "                         )\n",
    "dev_loader = DataLoader(dev_set, batch_size=hp.batch_size, shuffle=True,\n",
    "                        num_workers=hp.num_workers, pin_memory=hp.pin_memory\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd5dee48-3a8b-4509-a59b-b8a513fed6f0",
   "metadata": {
    "id": "bd5dee48-3a8b-4509-a59b-b8a513fed6f0"
   },
   "outputs": [],
   "source": [
    "attn = Attention(hp.bert_hidden_size, hp.decoder_hidden_size, hp.attention_hidden_size)\n",
    "decoder = Decoder(hp.bert_vocab_size, hp.decoder_input_size, hp.bert_hidden_size,\n",
    "                  hp.decoder_hidden_size, hp.n_layers, hp.dropout, attn)\n",
    "encoder = BertModel.from_pretrained(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6a1144c-86cd-47b7-8478-cbb0f73b9056",
   "metadata": {
    "id": "f6a1144c-86cd-47b7-8478-cbb0f73b9056"
   },
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder, decoder, hp.encoder_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abd9f8ac-f0b8-4be7-8be9-11609e236cff",
   "metadata": {
    "id": "abd9f8ac-f0b8-4be7-8be9-11609e236cff"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(decoder.parameters(), weight_decay=hp.weight_decay, lr=hp.lr, momentum=hp.momentum)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6632ab1-3b24-4b25-ac15-61f45ec4c927",
   "metadata": {
    "id": "a6632ab1-3b24-4b25-ac15-61f45ec4c927",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16ff6b7a-e7b1-4739-82ac-038af79a850b",
   "metadata": {
    "id": "16ff6b7a-e7b1-4739-82ac-038af79a850b"
   },
   "outputs": [],
   "source": [
    "train_losses, dev_losses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b679d3-4e0b-424c-818d-d337f89b82dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73b679d3-4e0b-424c-818d-d337f89b82dc",
    "outputId": "9d101bda-7285-456d-9dda-9a6ad471b81e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 143/1435 [00:47<07:08,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 142: Sentence Loss: 151.961, Word Loss: 6.987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 286/1435 [01:34<06:21,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 285: Sentence Loss: 146.593, Word Loss: 7.239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 429/1435 [02:22<05:33,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 428: Sentence Loss: 169.957, Word Loss: 7.156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 572/1435 [03:09<04:46,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 571: Sentence Loss: 154.620, Word Loss: 7.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 715/1435 [03:56<03:58,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 714: Sentence Loss: 134.145, Word Loss: 7.452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 858/1435 [04:44<03:11,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 857: Sentence Loss: 129.929, Word Loss: 7.425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 1001/1435 [05:31<02:24,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000: Sentence Loss: 170.456, Word Loss: 7.177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 1144/1435 [06:19<01:36,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1143: Sentence Loss: 166.606, Word Loss: 7.166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 1171/1435 [06:28<01:27,  3.02it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(hp.n_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, hp.clip)\n",
    "    dev_loss, bleu_score_i = eval(model, dev_loader, criterion)\n",
    "    print('End of epoch {}. Train loss is {:.3f}, Dev loss is {:.3f}, Bleu is {}'\n",
    "          .format(epoch+1, train_loss, dev_loss, bleu_score_i))\n",
    "    train_losses.append(train_loss)\n",
    "    dev_losses.append(dev_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8cda01-1bdf-42da-b1e3-5c81ef009682",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{}'.format(0.123242))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc68fe9-c034-457f-bb97-4afe7b60f730",
   "metadata": {
    "id": "bfc68fe9-c034-457f-bb97-4afe7b60f730"
   },
   "outputs": [],
   "source": [
    "torch.cat((torch.randn(51, 11, 123, 123), torch.randn(53, 13, 123, 123)), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27306bad-38e7-41ff-a26e-563d848d1025",
   "metadata": {
    "id": "bfc68fe9-c034-457f-bb97-4afe7b60f730"
   },
   "outputs": [],
   "source": [
    "torch.cat((torch.randn(51, 11, 123), torch.randn(53, 13, 123)), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36dbed1-8cae-4440-8bfe-139fc33e9929",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c256e-f08a-4f35-824d-7c814b5a9d09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "bertqg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "QA-nlp",
   "language": "python",
   "name": "qa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
