{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70f08c6-c086-4384-b0e4-7280e8afe8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==3.0.0\n",
    "# !pip install datasets\n",
    "# !pip install --user -U nltk\n",
    "# !python -m nltk.downloader punkt\n",
    "# !pip install git+git://github.com/adamnpeace/nlpt5.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0cab1a-1a4e-4333-9fd7-9b835c5bb601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, HfArgumentParser\n",
    "import nlp\n",
    "from pathlib import Path\n",
    "import json\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from nlpt5 import DataProcessor\n",
    "from nlpt5 import run_qg\n",
    "from nlpt5 import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa36e8d6-48ca-4f6a-bebe-9cb8a5719c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t5_qg_tokenizer/spiece.model',\n",
       " 't5_qg_tokenizer/special_tokens_map.json',\n",
       " 't5_qg_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "tokenizer.add_tokens(['<sep>', '<hl>'])\n",
    "tokenizer.save_pretrained('t5_qg_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a038ff7c-39aa-46db-b717-328d153af14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 512\n",
    "max_target_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe6042c-25b2-4bbf-981b-63d96d631dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 't5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0097163-aa87-4a82-a52a-06c97dce2025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not Path('data/dataset_train.pt').exists():\n",
    "    df_train = pd.read_json('data/data_train.json')\n",
    "    df_dev = pd.read_json('data/data_dev.json')\n",
    "\n",
    "    df_dev.columns\n",
    "\n",
    "    nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={\"filepath\": \"df_dev.csv\"})\n",
    "\n",
    "    df_dev = df_dev.rename(columns={'passages': 'source_text', 'clues': 'target_text'})\n",
    "    df_train = df_train.rename(columns={'passages': 'source_text', 'clues': 'target_text'})\n",
    "\n",
    "    df_dev = df_dev[['source_text', 'target_text']]\n",
    "    df_train = df_train[['source_text', 'target_text']]\n",
    "\n",
    "    df_dev.to_csv('df_dev.csv', index=False)\n",
    "    df_train.to_csv('df_train.csv', index=False)\n",
    "\n",
    "    ds_train = datasets.load_dataset('csv', data_files='df_train.csv', split=datasets.splits.Split('train'))\n",
    "    ds_dev = datasets.load_dataset('csv', data_files='df_dev.csv')\n",
    "\n",
    "    dataset_raw = datasets.load_dataset('csv', data_files={'train': 'df_train.csv', 'validation': 'df_dev.csv'})\n",
    "\n",
    "    processor = DataProcessor(tokenizer, model_type=model_type,\n",
    "                 max_source_length=max_source_length,\n",
    "                 max_target_length=max_target_length)\n",
    "\n",
    "    dataset = processor.process(dataset_raw)\n",
    "\n",
    "    columns = ['source_ids', 'target_ids', 'attention_mask']\n",
    "    dataset.set_format(type='torch', columns=columns)\n",
    "\n",
    "    torch.save(dataset['train'], 'data/dataset_train.pt')\n",
    "    torch.save(dataset['validation'], 'data/dataset_valid.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8284598a-6fe3-4077-9004-646dce157787",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a9256d-45ff-498b-b8d7-907c98a88094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 13:02:44 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "05/18/2021 13:02:45 - WARNING - nlpt5.run_qg -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "05/18/2021 13:02:45 - INFO - nlpt5.run_qg -   Training/evaluation parameters TrainingArguments(output_dir='t5-small-e2e-qg-7ktest', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, learning_rate=0.0001, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10, max_steps=-1, warmup_steps=0, logging_dir='runs/May18_13-02-44_monolith', logging_first_step=False, logging_steps=100, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, dataloader_drop_last=False)\n",
      "05/18/2021 13:02:45 - INFO - transformers.tokenization_utils_base -   Model name 't5_qg_tokenizer' not found in model shortcut name list (t5-small, t5-base, t5-large, t5-3b, t5-11b). Assuming 't5_qg_tokenizer' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "05/18/2021 13:02:45 - INFO - transformers.tokenization_utils_base -   Didn't find file t5_qg_tokenizer/tokenizer.json. We won't load it.\n",
      "05/18/2021 13:02:45 - INFO - transformers.tokenization_utils_base -   loading file t5_qg_tokenizer/spiece.model\n",
      "05/18/2021 13:02:45 - INFO - transformers.tokenization_utils_base -   loading file t5_qg_tokenizer/added_tokens.json\n",
      "05/18/2021 13:02:45 - INFO - transformers.tokenization_utils_base -   loading file t5_qg_tokenizer/special_tokens_map.json\n",
      "05/18/2021 13:02:45 - INFO - transformers.tokenization_utils_base -   loading file t5_qg_tokenizer/tokenizer_config.json\n",
      "05/18/2021 13:02:45 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "05/18/2021 13:02:45 - INFO - transformers.tokenization_utils -   Adding <sep> to the vocabulary\n",
      "05/18/2021 13:02:45 - INFO - transformers.tokenization_utils -   Adding <hl> to the vocabulary\n",
      "05/18/2021 13:02:45 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/valhalla/t5-small-e2e-qg/config.json from cache at /home/dojo/.cache/torch/transformers/beab4a6a48d1c4f4a667396444892a75f2f95b17010392027276c12addefa5fa.415e90660cccebe25315620884e999c97caf32207d04cf16ca1136f54c54a2c1\n",
      "05/18/2021 13:02:45 - INFO - transformers.configuration_utils -   Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 1.5,\n",
      "      \"max_length\": 256,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"generate questions: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 1.5,\n",
      "      \"max_length\": 256,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"generate questions: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 1.5,\n",
      "      \"max_length\": 256,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"generate questions: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 1.5,\n",
      "      \"max_length\": 256,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"generate questions: \"\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 32102\n",
      "}\n",
      "\n",
      "05/18/2021 13:02:45 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/valhalla/t5-small-e2e-qg/pytorch_model.bin from cache at /home/dojo/.cache/torch/transformers/01a91f53f6294e07fa74fcb12d6d885284fcd372e124c87bf56e517f7bebdcc5.7e5f666053a211b962912bf2caf98bec7d4874cab512ecfeea5509a152dd3b94\n",
      "05/18/2021 13:02:46 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "05/18/2021 13:02:46 - INFO - transformers.modeling_utils -   All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at valhalla/t5-small-e2e-qg.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "05/18/2021 13:02:46 - INFO - nlpt5.run_qg -   loading dataset\n",
      "05/18/2021 13:02:46 - INFO - nlpt5.run_qg -   finished loading dataset\n",
      "05/18/2021 13:02:49 - WARNING - transformers.trainer -   You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n",
      "05/18/2021 13:02:49 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "05/18/2021 13:02:49 - INFO - transformers.trainer -   ***** Running training *****\n",
      "05/18/2021 13:02:49 - INFO - transformers.trainer -     Num examples = 5744\n",
      "05/18/2021 13:02:49 - INFO - transformers.trainer -     Num Epochs = 10\n",
      "05/18/2021 13:02:49 - INFO - transformers.trainer -     Instantaneous batch size per device = 16\n",
      "05/18/2021 13:02:49 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "05/18/2021 13:02:49 - INFO - transformers.trainer -     Gradient Accumulation steps = 8\n",
      "05/18/2021 13:02:49 - INFO - transformers.trainer -     Total optimization steps = 440\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f308664d584ecf84ec6d44d046495e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=10.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ce53ea92494ad1a1e3ef0816db4016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=359.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584e38bd7f924ce0b2cf9f31d3b2a667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=359.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fba6c251fdb4f658464e4c854ce6e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=359.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2021 13:04:01 - INFO - transformers.trainer -   {'loss': 3.510298437476158, 'learning_rate': 7.727272727272727e-05, 'epoch': 2.267409470752089, 'step': 100}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc407e6cff8245f784b9c4093d17f911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=359.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c464ab5c8e4e3b8730e756e24ac5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=359.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a3f41eda4417>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mrun_qg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dev/.pyvenvs/qa-three/lib/python3.7/site-packages/nlpt5/run_qg.py\u001b[0m in \u001b[0;36mrun_qg\u001b[0;34m(args_dict)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"args.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/.pyvenvs/qa-three/lib/python3.7/site-packages/nlpt5/run_qg.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args_file)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         trainer.train(\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         )\n\u001b[1;32m    200\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/.pyvenvs/qa-three/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    461\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                 \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 if (step + 1) % self.args.gradient_accumulation_steps == 0 or (\n",
      "\u001b[0;32m~/dev/.pyvenvs/qa-three/lib/python3.7/site-packages/nlpt5/trainer.py\u001b[0m in \u001b[0;36m_training_step\u001b[0;34m(self, model, inputs, optimizer)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/.pyvenvs/qa-three/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/.pyvenvs/qa-three/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args_dict = {\n",
    "    \"model_name_or_path\": \"valhalla/t5-small-e2e-qg\",\n",
    "    \"model_type\": \"t5\",\n",
    "    \"tokenizer_name_or_path\": \"t5_qg_tokenizer\",\n",
    "    \"output_dir\": \"models/t5-small-e2e-qg-7k\",\n",
    "    \"train_file_path\": \"data/dataset_train.pt\",\n",
    "    \"valid_file_path\": \"data/dataset_valid.pt\",\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"num_train_epochs\": 10,\n",
    "    \"seed\": 42,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"evaluate_during_training\": False,\n",
    "    \"logging_steps\": 100,\n",
    "    \"overwrite_output_dir\": True\n",
    "}\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "run_qg(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8df96f-cd87-4adf-840a-8842692dc5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP (HFTF3)",
   "language": "python",
   "name": "venv-qa-three"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
